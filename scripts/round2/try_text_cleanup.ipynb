{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\code\\foe_foundry\\.venv\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from foe_foundry_nl.data.monsters2 import load_monsters\n",
    "from foe_foundry.creature_types import CreatureType\n",
    "import seaborn as sns\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import transformers\n",
    "from pathlib import Path\n",
    "import json as json\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dataclasses import asdict\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import os\n",
    "\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"hls\", len(CreatureType.all()))\n",
    "\n",
    "print(os.environ.get(\"hf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2203 monsters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9281e7a9fb7447bf84b36f3d0d2721e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Llama3:\n",
    "    def __init__(self, model_path: str, system_prompt: str):\n",
    "        self.model_id = model_path\n",
    "        self.system_prompt = system_prompt\n",
    "        self.pipeline = transformers.pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model_id,\n",
    "            token = os.environ.get(\"hf\"),\n",
    "            model_kwargs={\n",
    "                \"torch_dtype\": torch.bfloat16,\n",
    "            },\n",
    "            device=\"cuda\"\n",
    "        )\n",
    "        self.terminators = [\n",
    "            self.pipeline.tokenizer.eos_token_id,\n",
    "        ]\n",
    "\n",
    "    def get_response(\n",
    "          self, query\n",
    "      ):\n",
    "\n",
    "        conversation = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ]\n",
    "\n",
    "        prompt = self.pipeline.tokenizer.apply_chat_template(\n",
    "            conversation, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        outputs = self.pipeline(\n",
    "            prompt,\n",
    "            max_new_tokens=200,\n",
    "            eos_token_id=self.terminators,\n",
    "            pad_token_id=self.pipeline.tokenizer.eos_token_id,\n",
    "            num_beams=2,\n",
    "            early_stopping=True,\n",
    "            temperature=0.05,\n",
    "            top_p=0.5\n",
    "        )\n",
    "        response = outputs[0][\"generated_text\"][len(prompt):]\n",
    "        return response\n",
    "\n",
    "\n",
    "monsters = load_monsters()\n",
    "print(f\"Loaded {len(monsters)} monsters\")\n",
    "\n",
    "\n",
    "\n",
    "model_name = \"google/flan-t5-base\"\n",
    "# model_name = \"EleutherAI/gpt-j-6B\"\n",
    "# model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# model_name = \"tiiuae/falcon-7b-instruct\"\n",
    "# model_name = \"mistralai/Mistral-7B\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "system_prompt = (Path.cwd() / \"system_prompt.txt\").read_text()\n",
    "\n",
    "\n",
    "llama3 = Llama3(\"meta-llama/Llama-3.2-3B-Instruct\", system_prompt=system_prompt)\n",
    "\n",
    "\n",
    "\n",
    "rng = np.random.default_rng(20240711)\n",
    "\n",
    "for key, monster in monsters.items():\n",
    "    if monster.srd:\n",
    "        for paragraph_type, paragraph in monster.iter_paragraphs(rng):\n",
    "            path = Path.cwd().parent / \"data\" / \"5e_paragraphs\" / f\"{key}_{paragraph_type}.txt\"\n",
    "            path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            path.write_text(data=paragraph, encoding=\"utf-8\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
